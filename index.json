[{"content":"Search is an important functionality for any content heavy website. Our decision to use a static site for our recipe blog meant that adding complex features that involve server operations would be difficult to implement without relying on third-party tools. These features include comments and custom metric tracking.\n Luckily, this problem has been solved by a Javascript library called Lunr.js. Lunr provides a fast search experience for the user by processing JSON documents produced by static site generators like Hugo. The image above shows the JamilGhar\u0026rsquo;s search functionality in action.\nAdd a search page Inside the content folder, add a new file called search.md. Paste the following content and update the relevant fields.\n+++ title = \u0026quot;Find a page\u0026quot; date = \u0026quot;2020-11-12\u0026quot; aliases = [\u0026quot;search\u0026quot;] author = \u0026quot;Author Name\u0026quot; +++ {{\u0026lt; searchpage \u0026gt;}}  The second line will import a shortcode called searchpage.html, so let\u0026rsquo;s create that next.\nCreate search shortcode Inside the layouts directory, create a shortcodes directory if it doesn\u0026rsquo;t exist already. Inside shortcodes, create a new file called searchpage.html.\nThe path to your new file should look like:\n{{project directory}}/layouts/shortcodes/searchpage.html  Paste the following snippet inside searchpage.html\n\u0026lt;script src=\u0026quot;https://unpkg.com/lunr@2.3.8/lunr.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;/js/search.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;div\u0026gt; \u0026lt;input id=\u0026quot;search-input\u0026quot; type=\u0026quot;text\u0026quot; placeholder=\u0026quot;What are you looking for?\u0026quot; name=\u0026quot;search-input\u0026quot; class=\u0026quot;form-control\u0026quot;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;search-results\u0026quot; class=\u0026quot;container\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;  The page is fairly straightforward. We start by importing Lunr.js and a javascript file we will import next. After that we create a text input box where the user will enter search terms. There is no submit button here because the javascript function will trigger searches automatically on each keystroke.\nCreate search javascript file Inside the static folder, create a js folder if it doesn\u0026rsquo;t exist already. Create search.js inside the js directory.\nI relied on Matt Walters\u0026#39; implementation of search in Hugo for the Javascript portion. You\u0026rsquo;ll have to change the logic as needed based on your needs.\nvar idx = null; // Lunr index var resultDetails = []; // Will hold the data for the search results (titles and summaries) var $searchResults; // The element on the page holding search results var $searchInput; // The search box element window.onload = function () { // Set up for an Ajax call to request the JSON data file that is created by // Hugo's build process, with the template we added above var request = new XMLHttpRequest(); var query = ''; // Get dom objects for the elements we'll be interacting with $searchResults = document.getElementById('search-results'); $searchInput = document.getElementById('search-input'); request.overrideMimeType(\u0026quot;application/json\u0026quot;); request.open(\u0026quot;GET\u0026quot;, \u0026quot;/index.json\u0026quot;, true); // Request the JSON file created during build request.onload = function () { if (request.status \u0026gt;= 200 \u0026amp;\u0026amp; request.status \u0026lt; 400) { // Success response received in requesting the index.json file var documents = JSON.parse(request.responseText); // Build the index so Lunr can search it. The `ref` field will hold the URL // to the page/post. title, excerpt, and body will be fields searched. idx = lunr(function () { this.ref('title'); this.field('description'); this.field('href'); // Loop through all the items in the JSON file and add them to the index // so they can be searched. documents.forEach(function (doc) { this.add(doc); resultDetails[doc.title] = { 'href': doc.href, 'description': doc.description, }; }, this); }); } else { $searchResults.innerHTML = 'Error loading search results'; } }; request.onerror = function () { $searchResults.innerHTML = 'Error loading search results'; }; // Send the request to load the JSON request.send(); // Register handler for the search input field registerSearchHandler(); }; function registerSearchHandler() { $searchInput.oninput = function (event) { var query = event.target.value; var results = search(query); // Perform the search // Render search results renderSearchResults(results); // Remove search results if the user empties the search phrase input field if ($searchInput.value == '') { $searchResults.innerHTML = ''; } } } function renderSearchResults(results) { // Create a list of results var ul = document.createElement('ul'); if (results.length \u0026gt; 0) { results.forEach(function (result) { // Create result item var p = document.createElement('p'); p.innerHTML = '\u0026lt;a style=\u0026quot;color:#486b3d;font-weight: bolder;font-size: larger;\u0026quot; href=\u0026quot;' + resultDetails[result.ref].href + '\u0026quot;\u0026gt;' + result.ref + '\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;' + '\u0026lt;span style=\u0026quot;font-weight: lighter;font-size: large;padding-bottom: 1em;\u0026quot;\u0026gt;' + resultDetails[result.ref].description + '\u0026lt;/span\u0026gt;'; ul.appendChild(p); }); // Remove any existing content so results aren't continually added as the user types while ($searchResults.hasChildNodes()) { $searchResults.removeChild( $searchResults.lastChild ); } } else { $searchResults.innerHTML = 'No results found'; } // Render the list $searchResults.appendChild(ul); } function search(query) { return idx.search(query); }  The code is well documented and self-explanatory. It calls Lunr with the search term entered by the user and the raw data that the search will be performed against.\nPrepare JSON files For static sites, search is performed by parsing page content in JSON. We\u0026rsquo;ll start by exporting our pages using Hugo\u0026rsquo;s built-in data templates. Most theme\u0026rsquo;s support this functionality out of the box. You can find this by searching for \u0026ldquo;outputs\u0026rdquo; in the config.yaml file. You will need to add \u0026ldquo;JSON\u0026rdquo; to the outputs list.\nThe README for your chosen theme should also have notes on implementing search or exporting JSON.\nFor the two themes that I use, adding JSON looked like\n[outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;RSS\u0026quot;, \u0026quot;JSON\u0026quot;]  and\noutputs: home: - HTML - RSS - JSON # is necessary  ","permalink":"https://shariq1989.github.io/posts/search_for_hugo/","summary":"Search is an important functionality for any content heavy website. Our decision to use a static site for our recipe blog meant that adding complex features that involve server operations would be difficult to implement without relying on third-party tools. These features include comments and custom metric tracking.\n Luckily, this problem has been solved by a Javascript library called Lunr.js. Lunr provides a fast search experience for the user by processing JSON documents produced by static site generators like Hugo.","title":"Adding search functionality to a Hugo website"},{"content":"I really enjoy riding my Jamis Allegro A3 hybrid/fitness bike and exploring the areas east of Raleigh, NC. I\u0026rsquo;ll try to update this post with photos from recent rides.\n2021-10-03 White Oak Creek Greenway\n American Tobacco Trail\n 2021-09-12 Holly Springs, NC.\n   2021-09-05 American Tobacco Trail.\n 2021-08-15 Downtown Cary.\n 2021-08-05 American Tobacco Trail.\n 2021-08-01 Towing the kids for the first time.\n 2021-07-18 Cary Veterans Park.\n  2021-07-11 Umstead State Park near Cary, NC.\n  2021-07-06 Lake Crabtree in Cary, NC. First hour-long ride on my new Jamis bike.\n  2021-07-04 After a few months of riding my Mongoose Crossway, I have decided to upgrade to a nicer hybrid bike.\n 2021-06-29 My son has started to enjoy riding as well. He can easily ride 2-3 miles and I have to sprint to keep up.\n 2021-05-31 My first time exploring White Oak Creek Greenway. The Piedmont region of North Carolina (near Raleigh) has an excellent and ever-growing network of trails that are closed to vehicles. These trails are called greenways and are a great way to bike safely.\n ","permalink":"https://shariq1989.github.io/posts/biking/","summary":"I really enjoy riding my Jamis Allegro A3 hybrid/fitness bike and exploring the areas east of Raleigh, NC. I\u0026rsquo;ll try to update this post with photos from recent rides.\n2021-10-03 White Oak Creek Greenway\n American Tobacco Trail\n 2021-09-12 Holly Springs, NC.\n   2021-09-05 American Tobacco Trail.\n 2021-08-15 Downtown Cary.\n 2021-08-05 American Tobacco Trail.\n 2021-08-01 Towing the kids for the first time.\n 2021-07-18 Cary Veterans Park.","title":"Biking adventures through NC and beyond"},{"content":"At my work we use Personal Access Tokens (PATs) to authenticate to a Github repository (as opposed to using a password). I was having trouble cloning the repository and seeing the following error\n After browsing Stack Overflow posts, I realized that I didn\u0026rsquo;t set the right \u0026ldquo;scopes\u0026rdquo; or permissions for my token. I generated a new token but this time I made sure to set all the scopes under \u0026ldquo;repo\u0026rdquo;. After setting the appropriate scopes, I was able to access the repository with the new token.\n ","permalink":"https://shariq1989.github.io/posts/github_not_found/","summary":"At my work we use Personal Access Tokens (PATs) to authenticate to a Github repository (as opposed to using a password). I was having trouble cloning the repository and seeing the following error\n After browsing Stack Overflow posts, I realized that I didn\u0026rsquo;t set the right \u0026ldquo;scopes\u0026rdquo; or permissions for my token. I generated a new token but this time I made sure to set all the scopes under \u0026ldquo;repo\u0026rdquo;.","title":"GitHub 'repository not found' error when using an access token"},{"content":"One of the most important workflows for our food recipe blog (JamilGhar) is image optimization. My wife uploads images to our GitHub repository and, when prodded, I run a bash script that decreases image sizes and removes extraneous information from the files. The script is a constant work in progress and I have a few posts (1,2) about it on here.\nLately, I have been considering downloading the repository on a Raspberry Pi and running the script as a scheduled job. For that to happen, the job would have to run without errors. My updates today have been a step in that direction.\nSimplified .jpg/.jpeg logic I was handling .jpg and .jpeg files separately. It feels like a naive decision now because the code is less complicated and wordy now just by changing the references to *.jp*g (which can handle either format).\n# changed this find . -maxdepth 1 -type f -size +1M \\( -iname \\*.jpg -o -iname \\*.jpeg \\) -exec mv {} optimize/ \\; # to this find . -maxdepth 1 -type f -size +1M -name \u0026quot;*.jp*g\u0026quot; -exec mv {} optimize/ \\;  Terminated job if no images are found Since running the script was a manual process, I manually verified (or assumed) that there were files to be optimized. This procedure is not fail-safe and would not be suitable for an automated solution.\nFor this reason, I added an if-else block where I check for optimization candidates before running any operations.\ncount=`ls -1 *.jp*g 2\u0026gt;/dev/null | wc -l` if [ $count != 0 ] then ...  Complete script Undoubtedly, there are more changes to come in this script. For now, this feels enough for my purposes.\n# This script optimizes and resizes any images in the 'optimize' directory # navigate out to the images directory echo \u0026quot;------Navigating to images dir----------------\u0026quot; cd .. # move any JPG files larger than 1 MB to optimize dir echo \u0026quot;------Finding large files and moving them to optimize dir----------------\u0026quot; find . -maxdepth 1 -type f -size +1M -name \u0026quot;*.jp*g\u0026quot; -exec mv {} optimize/ \\; # go back to optimize dir echo \u0026quot;------Navigating to optimize dir----------------\u0026quot; cd optimize/ echo \u0026quot;------Are there files to modify?----------------\u0026quot; count=`ls -1 *.jp*g 2\u0026gt;/dev/null | wc -l` if [ $count != 0 ] then # backup images echo \u0026quot;------Yes, backing up large files----------------\u0026quot; find . -maxdepth 1 -type f -name \u0026quot;*.jp*g\u0026quot; -exec cp {} ../../backup/ \\; # remove file data, optimize file to reduce space echo \u0026quot;------Using jpegoptim to strip information----------------\u0026quot; jpegoptim *.jp*g --strip-all # reduce size echo \u0026quot;------Using mogrify to resize----------------\u0026quot; mogrify -resize 20% *.jp*g # move back to images dir echo \u0026quot;------Staging new images----------------\u0026quot; find . -maxdepth 1 -type f -name \u0026quot;*.jp*g\u0026quot; -exec mv {} ../ \\; # push changes echo \u0026quot;------Pushing changes----------------\u0026quot; # navigate out to root directory cd ../../.. git add * git commit -m \u0026quot;optimized images\u0026quot; git push else echo \u0026quot;------No images to optimize, aborting----------------\u0026quot; fi  ","permalink":"https://shariq1989.github.io/posts/optimizing_images_revisited/","summary":"One of the most important workflows for our food recipe blog (JamilGhar) is image optimization. My wife uploads images to our GitHub repository and, when prodded, I run a bash script that decreases image sizes and removes extraneous information from the files. The script is a constant work in progress and I have a few posts (1,2) about it on here.\nLately, I have been considering downloading the repository on a Raspberry Pi and running the script as a scheduled job.","title":"Optimizing images revisited"},{"content":"Google Search Console helps with understanding your website\u0026rsquo;s performance on the search engine. To track a website there, you must first verify domain ownership using the form shown below.\n The form asks us to add a TXT record for our URL. You\u0026rsquo;ll need to update this through your domain management tool. Since I use Namecheap to manage my domains, I\u0026rsquo;ll add the record there.\nAdding TXT record in Namecheap To access DNS settings, you\u0026rsquo;ll need to\n Hover over Account (top navigation bar) and click Dashboard Click manage on the right-hand side of the URL you intend on tracking Click Advanced DNS Click Add a new record As shown in the image below, select TXT Record in the first column for Type Add an @ in the Host column Paste the Google verification code you can copy from the form in Google Search Console into the Value column Leave TTL set to Automatic   Verification Within moments, you can go back to the Google Search Console verification click Verify. The verification should succeed, and you can start tracking your website.\n","permalink":"https://shariq1989.github.io/posts/google_search_console_verification/","summary":"Google Search Console helps with understanding your website\u0026rsquo;s performance on the search engine. To track a website there, you must first verify domain ownership using the form shown below.\n The form asks us to add a TXT record for our URL. You\u0026rsquo;ll need to update this through your domain management tool. Since I use Namecheap to manage my domains, I\u0026rsquo;ll add the record there.\nAdding TXT record in Namecheap To access DNS settings, you\u0026rsquo;ll need to","title":"Google Search Console verification for Namecheap"},{"content":"When my dad opened a new branch for his business, he opted for a new WordPress site which would be a copy of the site for the original location but with specific updated references like branch name, addresses and, phone numbers.\nHe uses GoDaddy as his host but this guide would apply to most web hosting solutions.\nCopying WordPress files When migrating or copying your WordPress files, you will first need to navigate to your host\u0026rsquo;s File Manager or File Navigator. As you can see below, GoDaddy provides the relevant tools right in their top navigation bar.\n Once there, navigate to the public_html directory. If you have multiple projects in there, you\u0026rsquo;ll want to enter into the specific project that you want to copy or migrate.\nYou\u0026rsquo;ll know you are in the right directory when you see the files below\n From here you can copy all your files to another instance with the same host or download them for deploying elsewhere. You can also use FileZilla (or similar FTP software) and connect to your host via FTP if you prefer and the host provides access (GoDaddy does).\nUpdate wp-config.php You will need to set up another database to support the new application. Before you do that, update the config file while you are still looking at the files.\nWe will need to edit wp-config.php which is located in the directory we found in the previous step. If your host allows it, you may edit the file in the file manager. GoDaddy doesn\u0026rsquo;t so I had to download the file, updated it and then upload the new file.\nThe following lines will need to be updated.\n  A database name. I use application initials followed by wp, ex. sjwp. This has to be unique.\n/** The name of the database for WordPress */ define('DB_NAME', 'NEW_DB_NAME');    A user name. I keep this the same as the db name. Ex. sjwp. This has to be unique.\n/** MySQL database username */ define('DB_USER', 'NEW_USR_NAME');    A password. I use a password manager to generate a long secure password.\n/** MySQL database password */ define('DB_PASSWORD', \u0026quot;NEW_PASSWD\u0026quot;);    Lastly, navigate to the following link to get a newly generated set of security keys and then overwrite the same section in the file.\nhttps://api.wordpress.org/secret-key/1.1/salt/\n  Create database The last step is to provision a database and user corresponding to the values you set in the previous step. In GoDaddy, this is done through the MySQL Databases option in CPanel. Your host may use a different interface for working with databases.\nCreating a new database using the interface is straightforward. Just remember to enter the database name you set in the previous section.\n Next, you\u0026rsquo;ll create a user (use the same values you set in the previous section). Lastly, we will associate the newly created database and user.\n And that\u0026rsquo;s it, your new WordPress instance has everything it needs\n A database and user WordPress files  Once you resolve the DNS settings, you can access the URL endpoint and go through installing the new instance.\nCopying a WordPress database I had a scenario in which I needed the new site to have the same data as the previous one. You may need this functionality if you are migrating to a different host. This is performed in GoDaddy through phpMyAdmin.\nThe first step is to export the old database by clicking Export in the navigation bar and then using the default options. Clicking Go will trigger the download of a SQL file.\n Lastly, you can import the exported SQL into the empty database we created earlier.\n ","permalink":"https://shariq1989.github.io/posts/migrating_wordpress_site/","summary":"When my dad opened a new branch for his business, he opted for a new WordPress site which would be a copy of the site for the original location but with specific updated references like branch name, addresses and, phone numbers.\nHe uses GoDaddy as his host but this guide would apply to most web hosting solutions.\nCopying WordPress files When migrating or copying your WordPress files, you will first need to navigate to your host\u0026rsquo;s File Manager or File Navigator.","title":"Copying or Migrating a WordPress Site"},{"content":"I help my dad maintain a WordPress site for his business. He recently opened a new location and needed a new WordPress site created for it. The only differences between the two sites are the branch name (ex. \u0026lsquo;Los Angeles\u0026rsquo; instead of \u0026lsquo;San Diego\u0026rsquo;) and phone number (\u0026lsquo;111-222-3333\u0026rsquo; instead of \u0026lsquo;222-333-4444\u0026rsquo;).\n The site is not stored in version control so downloading the source code, using an IDE to run find/replace and pushing updates wasn\u0026rsquo;t a straight-forward option. The tedious approach would be to update each reference on the site page by page in WordPress admin but that would take a long time and I would risk missing references or making mistakes.\nThankfully, after researching this problem I ran into a brilliant solution: a WordPress CLI (command-line interface) command called search-replace. It can be run on the server where the application is deployed.\nwp search-replace   Since I am comfortable with Linux, I decided to connect to the server from my machine and run the commands.\n As shown above, I first had to turn on SSH access with the web host and then create user credentials. I was then able to connect to the server using the following command\nssh \u0026lt;user\u0026gt;@\u0026lt;ip address\u0026gt; # example ssh user123@192.168.1.1  After entering yes to the fingerprint prompt, I was free to use the WP CLI. To ensure that I was making the changes I intended to make, I used the dry-run option. Using single quotes around the terms allowed me to use spaces. The first term is the one being searched for and the second is the replacing term. So in this example, Los Angeles will be replacing any instances of San Diego.\nwp search-replace 'San Diego' 'Los Angeles' --dry-run  As mentioned earlier, dry-run showed me that most of the changes will be in posts which is what I expected.\n I ran the command again but without the dry-run option.\nwp search-replace 'San Diego' 'Los Angeles'  \u0026hellip;and got an affirmative message.\n I was pleased to visit the website after and see that all the references to the previous branch name had been updated. Going forward, I intend on digging into WordPress CLI more because it seems to be a powerful and helpful tool.\n","permalink":"https://shariq1989.github.io/posts/mass_updates_wordpress_site/","summary":"I help my dad maintain a WordPress site for his business. He recently opened a new location and needed a new WordPress site created for it. The only differences between the two sites are the branch name (ex. \u0026lsquo;Los Angeles\u0026rsquo; instead of \u0026lsquo;San Diego\u0026rsquo;) and phone number (\u0026lsquo;111-222-3333\u0026rsquo; instead of \u0026lsquo;222-333-4444\u0026rsquo;).\n The site is not stored in version control so downloading the source code, using an IDE to run find/replace and pushing updates wasn\u0026rsquo;t a straight-forward option.","title":"Refactoring/Find-\u0026-Replace on a WordPress site"},{"content":"Over the years I have ended up with roughly a half dozen computers in various stages of decay. Two water-damaged Macbooks. An old HP laptop my dad handed down to me to \u0026ldquo;make faster and find a home for\u0026rdquo;. A gaming desktop from college that was caked in dust and shorted when powered up ten years later. One task I had to perform for all these machines was to collect and remove any personal data from internal storage (hard drives/solid-state drives).\nApparatus I am a fan of Sabrent\u0026rsquo;s external hard drive enclosures have used them for years. I have had one connected to a Raspberry Pi that serves as part of a media server solution and lives in a closet. I have not had to touch the enclosure in three years now, so it\u0026rsquo;s very reliable.\nI like that it can be used with 2.5-inch drives or the larger 3.5-inch ones as well. Since solid-state drives (SSDs) and hard disk drives (HDDs) both use SATA connections, the Sabrent enclosure will work with either type of drive. I did have to buy another enclosure that supports the PCI-E interface to read Macbook SSDs. Please use the link below to purchase if you\u0026rsquo;d like to support this blog.\n\u0026lt;strong\u0026gt;Sabrent 2.5/3.5 inch External HDD/SDD Enclosure\u0026lt;/strong\u0026gt;\n Wiping the drives Once the drive is in the enclosure and connected to my Pop!_OS laptop, I am ready to begin the wiping process.\nI first ran the following command to find the name of the drive on my machine.\nsudo fdisk -l   It was very important to correctly identify the drive you want to erase or you\u0026rsquo;ll accidentally wipe one of the drives connected internally to your machine. I know that my internal drive has a capacity of 500 GB whereas the drive I want to erase is 250 GB. The identifier for this drive is /dev/sdb.\nNext, I unmounted any partitions on the drive that are mounted on my computer. In the screenshot above, \u0026ldquo;/dev/sdb\u0026rdquo; refer to the drive and \u0026ldquo;/dev/sdb1\u0026rdquo; and \u0026ldquo;/dev/sdb2\u0026rdquo; refer to partitions. Note that the command to unmount a partition is umount and not unmount.\nsudo umount /dev/sdb1 sudo umount /dev/sdb2  Once the partitions are unmounted, I could start erasing the data. This is performed by writing \u0026ldquo;junk\u0026rdquo; data on the disk repeatedly. This will ideally prevent data recovery methods from recovering our erased personal data. The shred command used below begins the writing process and will write over the entire disk three times. The -v or verbose flag will display write progress as the command runs.\nsudo shred -v /dev/sdb  If successful, you will start seeing the command begin to write as shown below. It took a couple of hours for the three passes to complete on my laptop\n Once complete, if you run the fdisk command from earlier now, you\u0026rsquo;ll see that the partitions on that drive are now gone. To create new partitions, I needed to install gparted.\nsudo apt-get install gparted  Once installed, I started gparted using the Applications menu. I clicked the drop-down menu in the upper-right corner of the page and selected \u0026ldquo;/dev/sdb (232 GB)\u0026rdquo;. It displayed as entirely \u0026ldquo;unallocated\u0026rdquo;.\nNow you are ready to partition the drive. Use Device -\u0026gt; Create Partition Table\u0026hellip; to begin. You will see a scary prompt mentioning that any data on this drive will be erased. Ensure that this is the drive you want to wipe before continuing.\nOnce complete, use Partition -\u0026gt; New and then select the format of the partition you want (I use fat32) before kicking off the partitioning. Click the green checkmark and then set the file system you want. I use fat32 for Windows and ext4 for Linux disks. You can always change the file system later.\nAnd there we are! The drive has now been written on three times over, formatted and then partitioned properly. The drive can now be sold separately or go back into its mothership.\n ","permalink":"https://shariq1989.github.io/posts/wiping_disks/","summary":"Over the years I have ended up with roughly a half dozen computers in various stages of decay. Two water-damaged Macbooks. An old HP laptop my dad handed down to me to \u0026ldquo;make faster and find a home for\u0026rdquo;. A gaming desktop from college that was caked in dust and shorted when powered up ten years later. One task I had to perform for all these machines was to collect and remove any personal data from internal storage (hard drives/solid-state drives).","title":"Erasing personal data from hard drives or solid-state drives"},{"content":"This blog has been a nice source of motivation to continue improving my code and development processes. In my previous post, I discussed the process of writing a Bash script to resize and optimize images for publishing to our family\u0026rsquo;s recipe blog. I mentioned that one improvement I could make is to automate the process of identifying which files needed to be resized.\nThe original script I place large images into the \u0026ldquo;optimize\u0026rdquo; directory which are then backed up and processed when I run the following script. I have to do this every time new images are added to the blog (2-5 times a week)\n# This script optimizes and resizes any images in the optimize directory # backup images cp *.jpg ../../backup/ # remove file data, optimize file to reduce space jpegoptim *.jpg --strip-all # reduce size mogrify -resize 20% *.jpg # move back to images dir mv *.jpg ../ # push changes echo \u0026quot;------Pushing changes----------------\u0026quot; # navigate out to root directory cd ../../.. git add * git commit -m \u0026quot;optimized images\u0026quot; git push  Automate In my experience, process automation has looked something like the following\n Identify the manual process Write down the rules Automate the process based on the rules from step 2 Compare the input(s) and output(s) from the original process to those from the automated process If there is a difference found in step 4, go back to step 2  We followed this process when automating price quoting for a global shipping company. We were going to replace a large call center with a web service. It was tough to nail down because people and relationships were involved. Certain vendors had longstanding relationships with call centers. Prices didn\u0026rsquo;t always follow a mathematical equation.\nAutomation required that we had to draw a line somewhere, come up with that equation and stick to it. Custom discounts could be coded in, as well. The process led to more transparency and higher sales.\nThe improvement For my script, the rule was any JPG greater than 1 MB in the images directory. Instead of me moving the files, the script would identify files that match these criteria and move them for me.\nI added the following code to automate the moving process. I no longer have to take any steps apart from running the script. This means that the script can now be run periodically (cron job) or be triggered as part of continuous integration.\n# move any JPG files larger than 1 MB to optimize dir find . -maxdepth 1 -type f -size +1M -name '*.jpg' -exec mv {} optimize/ \\;  ","permalink":"https://shariq1989.github.io/posts/find_move/","summary":"This blog has been a nice source of motivation to continue improving my code and development processes. In my previous post, I discussed the process of writing a Bash script to resize and optimize images for publishing to our family\u0026rsquo;s recipe blog. I mentioned that one improvement I could make is to automate the process of identifying which files needed to be resized.\nThe original script I place large images into the \u0026ldquo;optimize\u0026rdquo; directory which are then backed up and processed when I run the following script.","title":"Smarter image optimization"},{"content":"When we first started adding photos of food to JamilGhar, we did not think about image size. The average image we posted to the blog was between 2 and 8 megabytes and the resolution was over 3000 x 3000 pixels. The image below is an example.\n For most blogs, images should be less than 1000 pixels in the longest direction and preferably less than 100 kB. Featured or \u0026ldquo;hero\u0026rdquo; images can be up to 500 kB if used sparingly.\nImage optimization research There are many ways to reduce the size of an image\n Within image editing software (Paint, GIMP, Photoshop, etc) Using online utilities Using command-line utilities (jpegoptim, optipng)  The first two options seemed tedious because they are hard to automate. Using command-line to resize an image means that I can write a bash script that can process a large number of images with the same number of clicks.\nCommand-line utilities jpegoptim This is a popular online utility for optimizing and compressing JPEG files. It is capable of accepting multiple images at a time. Below are two important commands for our use-case.\nOptimize JPEG images, stripping all non-essential data\njpegoptim --strip-all image1.jpeg image2.jpeg  Reduce to a specified size\njpegoptim --size=250k image1.jpeg image2.jpeg imageN.jpeg  mogrify This utility is similar to jpegoptim and can be used for resizing, cropping, blurring or flipping images. Here is the command for resizing an image by 50%.\nmagick mogrify -resize 50% rose.jpg  A script for optimizing images for blogging As mentioned in previous posts, my wife is responsible for adding content and I focus on meeting any technical needs. When adding a new recipe, she will upload 5-10 photos to the GitHub repository. Once or twice a week, I inspect the \u0026ldquo;images\u0026rdquo; directory for any files over 1 MB. New image files that she uploads are greater than that limit as shown in the image at the beginning of this post.\nI needed a script that would back up the original image, optimize and resize, and then place the newly generated image in the appropriate static/images directory. After processing, the backed-up original images and the processed versions would be pushed to the Git repository. Voila, a bash script called optimize was born.\nThe directory structure of JamilGhar\u0026rsquo;s image files is as follows. Any images that display on the blog are stored in the images directory. The original versions of these images are stored in the backup directory. Before running the optimize script, candidate images for resizing are moved to the optimize directory.\n- static - backup - images - optimize  Once large images (\u0026gt; 1 MB) have been moved to the optimize directory, the script is run. It will store a copy of the original image in the backup directory\n# backup images cp *.jpg ../../backup/  Use jpegoptim to remove metadata and mogrify to reduce image size by 80%.\n# remove file data, optimize file to reduce space jpegoptim *.jpg --strip-all # reduce size mogrify -resize 20% *.jpg  Move the newly generated image back to the images directory.\n# move back to images dir mv *.jpg ../  Push the backed-up files and processed copies to the GitHub repository.\n# push changes echo \u0026quot;------Pushing changes----------------\u0026quot; # navigate out to root directory cd ../../.. git add * git commit -m \u0026quot;optimized images\u0026quot; git push  The Result The image below is a resized version of the one at the beginning of this post. The resolution is decreased to a third of the original value without sacrificing much quality. The size decreased by a staggering 94.7%! The script allows us to resize any number of images with the same effort.\n Future Considerations Moving the image files to the optimize directory is still a manual step. I did this to ensure that I got to have the final say on which images are adjusted. This is at the cost of having huge images on the website until I get a chance to resize them. I believe this flow could be improved and fully automated by hooking into a Github Action or Netlify\u0026rsquo;s continuous integration flow. It also only handles JPG files at the moment so extending the script to handle PNG\u0026rsquo;s and GIF\u0026rsquo;s would be beneficial.\n","permalink":"https://shariq1989.github.io/posts/optimizing_images/","summary":"When we first started adding photos of food to JamilGhar, we did not think about image size. The average image we posted to the blog was between 2 and 8 megabytes and the resolution was over 3000 x 3000 pixels. The image below is an example.\n For most blogs, images should be less than 1000 pixels in the longest direction and preferably less than 100 kB. Featured or \u0026ldquo;hero\u0026rdquo; images can be up to 500 kB if used sparingly.","title":"Optimizing Images for the Web"},{"content":"Adding or updating posts in Hugo is easy. After all, content management is one of the framework\u0026rsquo;s primary features. My wife has little interest in understanding the software development side of blogging. Even she finds it effortless to add blog posts in markdown and half-a-dozen images per post through Github.\nDirectory structure In Hugo, posts go into the content folder which will be in the root directory of your site. Some theme\u0026rsquo;s may have varying patterns.\nAs shown below, create a post directory under the content directory.\nfood-blog - archetypes - content - post - data - layouts - resources - static - themes config.toml  Navigate to the new post directory. Now copy the posts in Anatole\u0026rsquo;s example site over to our newly created directory.\ncd content/post cp ../../themes/anatole/exampleSite/content/post/* .  Navigate back to the root directory of the Hugo site and start the Hugo server\ncd ../.. hugo serve  If you open the site in a browser now, you will see that the posts you added have now been loaded! If you click on a post, you will see what is called the single view in Hugo. You are seeing the entirely of the post you clicked on, as opposed to the list view which contains snippets of each post.\n We have learned so far that you can add new posts to Hugo just by adding them to the content/post directory. Lets see what a post looks like in Anatole. Posts in other themes will be very similar. Below is the Markdown code for one of the posts in Anatole\u0026rsquo;s example site.\nThe tags in the beginning such as author are self-explanatory. Try changing the values and seeing how the UI reacts. Text after the second '+++', is the blog post\u0026rsquo;s content in Markdown. I keep this cheatsheet handy for Markdown syntax.\n ","permalink":"https://shariq1989.github.io/posts/adding_content/","summary":"Adding or updating posts in Hugo is easy. After all, content management is one of the framework\u0026rsquo;s primary features. My wife has little interest in understanding the software development side of blogging. Even she finds it effortless to add blog posts in markdown and half-a-dozen images per post through Github.\nDirectory structure In Hugo, posts go into the content folder which will be in the root directory of your site. Some theme\u0026rsquo;s may have varying patterns.","title":"Creating blog posts in Hugo"},{"content":"Create a Hugo site After installing Hugo, I was ready to start on the blog. I have a directory (or \u0026ldquo;folder\u0026rdquo; for Windows folks) on my laptop for software development projects. Most of these projects are also hosted in my Github account.\nI navigated to that directory and created a folder called food-blog. Inside food-blog, I ran Hugo\u0026rsquo;s command to start a new site.\nhugo new site food-blog  In the screenshot below, you can see that the new Hugo site was created successfully.\n You may wonder why I created the Hugo site inside a directory with the same name (food-blog/food-blog). This is because I like to store files that I do not want to check in to version control in this root directory. If you want to avoid this pattern, you can have these files in the Hugo site root and added to the .gitignore file.\nChoose a Theme As suggested by Hugo when you created the new site, you may now look for and implement a theme. Hugo has an extensive collection of themes available for free. Each will have its own set of instructions for setup and configuration. Not all themes offer the same features, so it is good to think about the functionality you need in your site. Other factors I consider when choosing a theme are\n Number of stars on Github Open issues on Github Time since the last commit Testing/QA  These factors will indicate how well-maintained the theme is. If you have an issue down the road, you will want the theme\u0026rsquo;s maintainer to be engaged. The number of stars shows the theme\u0026rsquo;s popularity - if it\u0026rsquo;s used enough, chances are that quite a few issues have already been addressed and that there is a larger pool of contributors. Testing/QA helps ensure that new commits do not include vulnerabilities and do not break existing functionality.\nTwo well-supported and feature-rich themes that I like are Anatole and PaperMod. You can make changes to a theme to support your use-case, so it\u0026rsquo;s fine to choose a theme that loosely provides the structure you want.\n  JamilGhar uses the Anatole theme    This site uses the PaperMod theme  Install the Theme To begin installing the theme, navigate to the newly created Hugo site\u0026rsquo;s root directory. Now you can initialize the Git repository here and pull down the theme as a submodule.\nA Git submodule allows you to pull a separate repository into a project - which is a great model for importing libraries. A theme in Hugo is similar to importing a library.\ngit init git submodule add https://github.com/lxndrblz/anatole.git themes/anatole   The submodule has now been downloaded and is ready for configuration.\nConfiguring the Theme Themes can vary in how they are configured. You will need to read through these settings in the theme\u0026rsquo;s documentation (often found in the theme\u0026rsquo;s Github README). Anatole\u0026rsquo;s maintainer did a great job describing the setup process.\nThemes are mainly configured by updating the config.toml or config.yml file located in the root directory. You may often find a sample config file in the theme\u0026rsquo;s repository. You can compare the contents of this file to the demo site to learn more about the way the theme works.\nI overwrote the default Hugo config file with the one provided in Anatole\u0026rsquo;s repository.\nmv themes/anatole/exampleSite/config.toml .  Feel free to personalize the information in this file before moving onto the next step.\nBuild and Launch! Run the following command in the Hugo site\u0026rsquo;s base directory to build and run the site locally.\nhugo serve   In the output, you\u0026rsquo;ll see a path http://localhost:1313. Click the URL or paste it into your browser to view your site.\n The images do not appear because they do not exist in your Hugo project. You will have to add your own. At this point, you can start adding posts or customizing the site.\n","permalink":"https://shariq1989.github.io/posts/setting_up_hugo/","summary":"Create a Hugo site After installing Hugo, I was ready to start on the blog. I have a directory (or \u0026ldquo;folder\u0026rdquo; for Windows folks) on my laptop for software development projects. Most of these projects are also hosted in my Github account.\nI navigated to that directory and created a folder called food-blog. Inside food-blog, I ran Hugo\u0026rsquo;s command to start a new site.\nhugo new site food-blog  In the screenshot below, you can see that the new Hugo site was created successfully.","title":"Adding a theme in Hugo"},{"content":"To begin working on the blog, I needed to install Hugo first. My laptop runs Pop_OS!, a Linux distribution based on Ubuntu. There are quite a few ways to install Hugo and I chose to install the official package using apt-get.\nsudo apt-get install hugo  Once the install was complete, I verified the install\nhugo version  As shown in the screenshot below, Hugo version 0.74.3 installed successfully on my machine.\n ","permalink":"https://shariq1989.github.io/posts/installing_hugo/","summary":"To begin working on the blog, I needed to install Hugo first. My laptop runs Pop_OS!, a Linux distribution based on Ubuntu. There are quite a few ways to install Hugo and I chose to install the official package using apt-get.\nsudo apt-get install hugo  Once the install was complete, I verified the install\nhugo version  As shown in the screenshot below, Hugo version 0.74.3 installed successfully on my machine.","title":"Installing Hugo in Linux (Ubuntu)"},{"content":"My wife and I have always wanted to work on an online project together. She is creative, and I can figure out technology. A multitude of reasons pushed us to start a blog in early 2021.\n My hunger for a software-dev-related side project A desire to digitalize our growing three-ring recipe binder Repeated requests for the same recipes (mostly Thai Red Curry and Butter Chicken) from friends and family  It was easy to devise requirements as we thought through different features (blog setup, adding posts and pictures, monetization, etc.).\n Intuitive and easy CRUD-ing of recipes Low reliance on third-party plugins A fast, secure, and dependable web framework Rapid automated deployments Intuitive and quick initial set up Decently-sized, engaged developer community  Choices I did some research and considered the pros and cons of using different blogging stacks with my wife. These are just some of the many good choices out there, and there are fantastic blogs using each one of them. We ended up choosing the most fun and appropriate solution for us.\nCMS (WordPress, Drupal, Joomla, \u0026hellip;) Content management systems have been dominant players in the web dev space for a long time. Between taking Jenn Kramer\u0026rsquo;s CMS courses and troubleshooting a WordPress site for my dad\u0026rsquo;s work, I am comfortable with the technology. Reliance on closed-source third-party plugins, use of PHP, security vulnerabilities and website size turned me off from this option.\nDjango/Wagtail I have been enamored with all things Django for a few years. I follow the maintainers on Twitter, listen to the official podcast and watch conference talks on YouTube. I could have developed and managed a blog using it but I felt like the learning curve would drive my wife away. At the time, we just wanted to throw some images and text on a page for people to see.\nSaaS (Wix, Squarespace) I usually recommend these solutions to friends that want a website but do not want to develop with code. These would be costlier and wouldn\u0026rsquo;t scratch my development itch.\nJamstack This newer paradigm is getting popular for producing simple, fast and secure static sites. The downside is not being able to add features like authentication, comments, search, etc.\nSolution Hugo, Github, Netlify Few solutions are perfect but we felt that this stack checked most of our boxes. Since we plan to post recipes with images, markdown will be a good solution. My wife seems to enjoy the syntax more than working in plain HTML. Hooking our Github repository into Netlify\u0026rsquo;s continuous integration/continuous deployment system meant that after my wife adds a new recipe on Github, the recipe would be deployed to the website in seconds without any extra intervention.\n ","permalink":"https://shariq1989.github.io/posts/intro/","summary":"My wife and I have always wanted to work on an online project together. She is creative, and I can figure out technology. A multitude of reasons pushed us to start a blog in early 2021.\n My hunger for a software-dev-related side project A desire to digitalize our growing three-ring recipe binder Repeated requests for the same recipes (mostly Thai Red Curry and Butter Chicken) from friends and family  It was easy to devise requirements as we thought through different features (blog setup, adding posts and pictures, monetization, etc.","title":"We need a blog stack"}]